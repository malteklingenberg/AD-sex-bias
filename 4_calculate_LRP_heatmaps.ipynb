{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "import nibabel\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from innvestigator import InnvestigateModel\n",
    "from nmm_mask_areas import all_areas\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#### file paths ####\n",
    "####################\n",
    "\n",
    "## INPUT FILE PATHS\n",
    "# this notebook assumes that both paths given below contain subfolders for the different data splits,\n",
    "# with the folder names given in the list 'splits'\n",
    "#   - data_base_path/[split] should contain the h5 files (only holdout needed here)\n",
    "#   - data_base_path should also contain [split]_test.csv (from 1_create_dataset_splits_stratified)\n",
    "#   - models_base_path/[split] should contain the trained models (from 2_train_models_multiGPU)\n",
    "data_base_path = '/path/to/data'\n",
    "models_base_path = '/path/to/models'\n",
    "splits = [splits = ['split_0', 'split_1', 'split_2']]\n",
    "# the NMM mask, which must be rescaled to the input image dimensions\n",
    "nmm_mask_path_scaled = '/path/to/nmm_mask_rescaled.nii'\n",
    "\n",
    "## OUTPUT\n",
    "# base path for the LRP heatmaps\n",
    "# the files will be created as '[lrp_path]/[split]/[repeat]/[subject id].nii'\n",
    "lrp_path = '/path/to/lrp'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel3D(nn.Module):\n",
    "    def __init__(self, dropout=0.4, dropout2=0.4):\n",
    "        nn.Module.__init__(self)\n",
    "        self.Conv_1 = nn.Conv3d(1, 8, 3)\n",
    "        self.Conv_1_bn = nn.BatchNorm3d(8)\n",
    "        self.Conv_1_mp = nn.MaxPool3d(2)\n",
    "        self.Conv_2 = nn.Conv3d(8, 16, 3)\n",
    "        self.Conv_2_bn = nn.BatchNorm3d(16)\n",
    "        self.Conv_2_mp = nn.MaxPool3d(3)\n",
    "        self.Conv_3 = nn.Conv3d(16, 32, 3)\n",
    "        self.Conv_3_bn = nn.BatchNorm3d(32)\n",
    "        self.Conv_3_mp = nn.MaxPool3d(2)\n",
    "        self.Conv_4 = nn.Conv3d(32, 64, 3)\n",
    "        self.Conv_4_bn = nn.BatchNorm3d(64)\n",
    "        self.Conv_4_mp = nn.MaxPool3d(3)\n",
    "        self.dense_1 = nn.Linear(2304, 128)\n",
    "        self.dense_2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.Conv_1_bn(self.Conv_1(x)))\n",
    "        x = self.Conv_1_mp(x)\n",
    "        x = self.relu(self.Conv_2_bn(self.Conv_2(x)))\n",
    "        x = self.Conv_2_mp(x)\n",
    "        x = self.relu(self.Conv_3_bn(self.Conv_3(x)))\n",
    "        x = self.Conv_3_mp(x)\n",
    "        x = self.relu(self.Conv_4_bn(self.Conv_4(x)))\n",
    "        x = self.Conv_4_mp(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.dense_1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense_2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ADNI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(subset):\n",
    "    for i in range(len(subset)):\n",
    "        subset[i] -= np.min(subset[i])\n",
    "        subset[i] /= np.max(subset[i])\n",
    "    return subset\n",
    "    \n",
    "def load_data(skip_train=True, skip_val=True, skip_test=False, dtype=np.float32):\n",
    "    \"\"\" Load hdf5 files and extract columns. \"\"\"\n",
    "    X_train, y_train, X_val, y_val, X_holdout, y_holdout = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    # train\n",
    "    if not skip_train:\n",
    "        train_h5_ = h5py.File(train_h5, 'r')\n",
    "        X_train, y_train = train_h5_['X'], train_h5_['y']\n",
    "        X_train = np.expand_dims(np.array(X_train, dtype=dtype), 1)\n",
    "        X_train = min_max_normalization(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        print(\"Total training set length: {}\".format(len(y_train)))\n",
    "        print(\"Number of healthy controls: {}\".format(len(np.array(y_train)[np.array(y_train)==0.])))\n",
    "        print(\"Number of AD patients: {}\".format(len(np.array(y_train)[np.array(y_train)==1.])))\n",
    "    if not skip_val:\n",
    "        # val\n",
    "        val_h5_ = h5py.File(val_h5, 'r')\n",
    "        X_val, y_val = val_h5_['X'], val_h5_['y']\n",
    "        X_val = np.expand_dims(np.array(X_val, dtype=dtype), 1)\n",
    "        X_val = min_max_normalization(X_val)\n",
    "        y_val = np.array(y_val)\n",
    "        print(\"Total validation set length: {}\".format(len(y_val)))\n",
    "    if not skip_test:\n",
    "        # test\n",
    "        holdout_h5_ = h5py.File(holdout_h5, 'r')\n",
    "        X_holdout, y_holdout = holdout_h5_['X'], holdout_h5_['y']\n",
    "        X_holdout = np.expand_dims(np.array(X_holdout, dtype=dtype), 1)\n",
    "        X_holdout = min_max_normalization(X_holdout)\n",
    "        y_holdout = np.array(y_holdout)\n",
    "        print(\"Total test set length: {}\".format(len(y_holdout)))\n",
    "   \n",
    "    return X_train, y_train, X_val, y_val, X_holdout, y_holdout\n",
    "\n",
    "\n",
    "def load_nifti(file_path, mask=None, z_factor=None, remove_nan=True):\n",
    "    \"\"\"Load a 3D array from a NIFTI file.\"\"\"\n",
    "    img = nibabel.load(file_path)\n",
    "    struct_arr = np.array(img.get_data())\n",
    "\n",
    "    if remove_nan:\n",
    "        struct_arr = np.nan_to_num(struct_arr)\n",
    "    if mask is not None:\n",
    "        struct_arr *= mask\n",
    "    if z_factor is not None:\n",
    "        struct_arr = np.around(zoom(struct_arr, z_factor), 0)\n",
    "\n",
    "    return struct_arr\n",
    "\n",
    "\n",
    "def save_nifti(file_path, struct_arr):\n",
    "    \"\"\"Save a 3D array to a NIFTI file.\"\"\"\n",
    "    img = nibabel.Nifti1Image(struct_arr, np.eye(4))\n",
    "    nibabel.save(img, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmm_mask = load_nifti(nmm_mask_path_scaled)\n",
    "\n",
    "mri_shape = (182, 218, 182)\n",
    "\n",
    "# all_areas holds the area name and a tuple with the minimum \n",
    "# idx in the NMM mask and the maximum idx in the NMM mask belonging to that area\n",
    "area_masks = {k: None for k in all_areas.keys()}\n",
    "for name, (min_idx, max_idx) in all_areas.items():\n",
    "    area_mask = np.zeros(mri_shape)\n",
    "    area_mask[np.logical_and(nmm_mask>=min_idx, nmm_mask<=max_idx)] = 1\n",
    "    area_masks[name] = area_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LRP on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LRP(net, image_tensor):\n",
    "    return net.innvestigate(in_tensor=image_tensor, rel_for_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    print(\"#############################\")\n",
    "    print(\"##### split {}\".format(split))\n",
    "    print(\"#############################\\n\")\n",
    "    \n",
    "    #######################################\n",
    "    ##### load test data and subject info\n",
    "    #######################################\n",
    "    holdout_h5 = '{}/{}/ADNI_3T_AD_CN_holdout.h5'.format(data_base_path, split)\n",
    "    _, _, _, _, X_holdout, y_holdout = load_data()\n",
    "    \n",
    "    subjects_csv = '{}/{}_test.csv'.format(data_base_path, split[:-4])\n",
    "    subjects = pd.read_csv(subjects_csv)  \n",
    "    \n",
    "    for repeat in range(5):\n",
    "        print(\"starting split {}, repeat {}\".format(split, repeat))\n",
    "        ##################\n",
    "        ##### load model\n",
    "        ##################\n",
    "        model_path = '{}/{}/trial_{}_BEST_ITERATION.h5'.format(models_base_path, split, repeat)\n",
    "        \n",
    "        device = 0\n",
    "        net = ClassificationModel3D()\n",
    "        net.cuda(device)\n",
    "\n",
    "        state_dict = torch.load(model_path, map_location='cpu')\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove \"module.\" prefix (due to nn.DataParallel)\n",
    "            new_state_dict[name] = v\n",
    "        \n",
    "        net.load_state_dict(new_state_dict)\n",
    "        net.eval()\n",
    "        net = torch.nn.Sequential(net, torch.nn.Softmax(dim=1))\n",
    "        inn_model = InnvestigateModel(net, lrp_exponent=1,\n",
    "                                          method=\"b-rule\",\n",
    "                                          beta=0, epsilon=1e-6).cuda(device)\n",
    "        inn_model.eval();\n",
    "        \n",
    "        #######################\n",
    "        ##### reset variables\n",
    "        #######################\n",
    "        cases = [\"AD\", \"HC\", \"TP\", \"TN\", \"FP\", \"FN\"]\n",
    "        mean_maps_LRP = {case: np.zeros(mri_shape) for case in cases}\n",
    "        rs_per_area_LRP = {case: {k: [] for k in all_areas.keys()} for case in cases}\n",
    "        counts = {case: 0 for case in cases}\n",
    "        area_sizes = {k: 0 for k in all_areas.keys()}\n",
    "        \n",
    "        ##########################\n",
    "        ##### calculate heatmaps\n",
    "        ##########################\n",
    "        heatmaps_path = '{}/{}/{}'.format(lrp_path, split, repeat)\n",
    "        os.makedirs(heatmaps_path, exist_ok=True)\n",
    "        num_samples = len(X_holdout)\n",
    "        ad_score_list = []\n",
    "\n",
    "        for i, (image, label) in enumerate(zip(X_holdout, y_holdout)):\n",
    "            image_tensor = torch.Tensor(image[None]).cuda(device)   \n",
    "            AD_score, LRP_map = run_LRP(inn_model, image_tensor)\n",
    "            AD_score = AD_score[0][1].detach().cpu().numpy()\n",
    "            LRP_map = LRP_map.detach().numpy().squeeze()\n",
    "            ad_score_list.append(AD_score)\n",
    "    \n",
    "            # save individual heatmap\n",
    "            subject = subjects.at[i, 'SUBJECT']\n",
    "            save_nifti(os.path.join(heatmaps_path, \"{}.nii\".format(subject)), LRP_map)\n",
    "    \n",
    "            true_case = \"AD\" if label else \"HC\"\n",
    "            if AD_score.round() and label:\n",
    "                case = \"TP\"\n",
    "            elif AD_score.round() and not label:\n",
    "                case = \"FP\"\n",
    "            elif not AD_score.round() and label:\n",
    "                case = \"FN\"\n",
    "            elif not AD_score.round() and not label:\n",
    "                case = \"TN\"\n",
    "    \n",
    "            mean_maps_LRP[case] += LRP_map\n",
    "            counts[case] += 1\n",
    "            mean_maps_LRP[true_case] += LRP_map\n",
    "            counts[true_case] += 1\n",
    "    \n",
    "            for name, (min_idx, max_idx) in all_areas.items():\n",
    "                area_mask = area_masks[name]\n",
    "                summed_LRP = (LRP_map * area_mask).sum()\n",
    "        \n",
    "                # Keep index in test set for identification\n",
    "                rs_per_area_LRP[case][name].append((i, summed_LRP))\n",
    "                rs_per_area_LRP[true_case][name].append((i, summed_LRP))\n",
    "        \n",
    "                if i < 1:\n",
    "                    area_size = area_mask.sum()\n",
    "                    area_sizes.update({name:area_size})\n",
    "    \n",
    "            print(\"Completed {0:3.2f}%  \\r\".format(100*(i+1)/num_samples), end=\"\")\n",
    "        \n",
    "        #####################\n",
    "        ##### save heatmaps\n",
    "        #####################\n",
    "        print(\"now saving heatmaps, case counts:\")\n",
    "        print(counts)\n",
    "        for case in cases:\n",
    "            mean_maps_LRP[case] /= counts[case]\n",
    "            save_nifti(os.path.join(heatmaps_path, \"LRP_{case}.nii\".format(case=case)),\n",
    "                       mean_maps_LRP[case])\n",
    "            with open(os.path.join(heatmaps_path, \"LRP_area_evdcs_{case}.pkl\".format(case=case)), 'wb') as file:\n",
    "                pickle.dump(rs_per_area_LRP[case], file)\n",
    "\n",
    "        with open(os.path.join(heatmaps_path, \"area_sizes.pkl\"), 'wb') as file:\n",
    "            pickle.dump(area_sizes, file)\n",
    "\n",
    "        np.savetxt(os.path.join(heatmaps_path, \"ad_scores.txt\"), ad_score_list)\n",
    "        \n",
    "        print(\"done with split {}, repeat {}\\n\\n\".format(split, repeat))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
