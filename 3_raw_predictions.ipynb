{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# load functions from nitorch\n",
    "from nitorch.transforms import  ToTensor\n",
    "from nitorch.metrics import balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#### file paths ####\n",
    "####################\n",
    "\n",
    "## INPUT FILE PATHS\n",
    "# this notebook assumes that both paths given below contain subfolders for the different data splits,\n",
    "# with the folder names given in the list 'splits'\n",
    "#   - data_base_path/[split] should contain the h5 files (only holdout needed here)\n",
    "#   - models_base_path/[split] should contain the trained models (from 2_train_models_multiGPU)\n",
    "data_base_path = '/path/to/data'\n",
    "models_base_path = '/path/to/models'\n",
    "splits = ['split_0', 'split_1', 'split_2']\n",
    "\n",
    "## OUTPUT\n",
    "# a file called \"raw_pred.pkl\" will be created in each models_base_path/[split] subfolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "b = 4 # batch size\n",
    "num_classes = 2\n",
    "\n",
    "dtype = np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER\n",
    "class ClassificationModel3D(nn.Module):\n",
    "    def __init__(self, dropout=0.4, dropout2=0.4):\n",
    "        nn.Module.__init__(self)\n",
    "        self.Conv_1 = nn.Conv3d(1, 8, 3)\n",
    "        self.Conv_1_bn = nn.BatchNorm3d(8)\n",
    "        self.Conv_1_mp = nn.MaxPool3d(2)\n",
    "        self.Conv_2 = nn.Conv3d(8, 16, 3)\n",
    "        self.Conv_2_bn = nn.BatchNorm3d(16)\n",
    "        self.Conv_2_mp = nn.MaxPool3d(3)\n",
    "        self.Conv_3 = nn.Conv3d(16, 32, 3)\n",
    "        self.Conv_3_bn = nn.BatchNorm3d(32)\n",
    "        self.Conv_3_mp = nn.MaxPool3d(2)\n",
    "        self.Conv_4 = nn.Conv3d(32, 64, 3)\n",
    "        self.Conv_4_bn = nn.BatchNorm3d(64)\n",
    "        self.Conv_4_mp = nn.MaxPool3d(3)\n",
    "        self.dense_1 = nn.Linear(2304, 128)\n",
    "        self.dense_2 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.Conv_1_bn(self.Conv_1(x)))\n",
    "        x = self.Conv_1_mp(x)\n",
    "        x = self.relu(self.Conv_2_bn(self.Conv_2(x)))\n",
    "        x = self.Conv_2_mp(x)\n",
    "        x = self.relu(self.Conv_3_bn(self.Conv_3(x)))\n",
    "        x = self.Conv_3_mp(x)\n",
    "        x = self.relu(self.Conv_4_bn(self.Conv_4(x)))\n",
    "        x = self.Conv_4_mp(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.dense_1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "    \n",
    "# DATASET\n",
    "class ADNIDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None, target_transform=None, mask=None, z_factor=None, dtype=np.float32, num_classes=2):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.mask = mask\n",
    "        self.z_factor = z_factor\n",
    "        self.dtype = dtype\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label_tensor = np.zeros(shape=(self.num_classes,))\n",
    "        label = self.y[idx] >= 0.5\n",
    "        label = torch.LongTensor([label])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\"image\" : image,\n",
    "                 \"label\" : label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    ################ LOAD DATA\n",
    "    \n",
    "    holdout_h5 = '{}/{}/ADNI_3T_AD_CN_holdout.h5'.format(data_base_path, split)\n",
    "    holdout_h5_ = h5py.File(holdout_h5, 'r')\n",
    "    \n",
    "    X_holdout, y_holdout = holdout_h5_['X'], holdout_h5_['y']\n",
    "    \n",
    "    X_holdout = np.array(X_holdout)\n",
    "    y_holdout = np.array(y_holdout)\n",
    "     \n",
    "    for i in range(len(X_holdout)):\n",
    "        X_holdout[i] -= np.min(X_holdout[i])\n",
    "        X_holdout[i] /= np.max(X_holdout[i])\n",
    "    \n",
    "    adni_data_test = ADNIDataset(X_holdout, y_holdout, transform=transforms.Compose([ToTensor()]), dtype=dtype)\n",
    "    \n",
    "    ############### LOAD MODELS\n",
    "    \n",
    "    model_path = '{}/{}'.format(models_base_path, split)\n",
    "    models = []\n",
    "    for i in range(5):\n",
    "        filename = \"/trial_{}_BEST_ITERATION.h5\".format(i)\n",
    "        net = ClassificationModel3D()\n",
    "        \n",
    "        state_dict = torch.load(model_path + filename)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove \"module.\" prefix (due to nn.DataParallel)\n",
    "            new_state_dict[name] = v\n",
    "        \n",
    "        net.load_state_dict(new_state_dict)\n",
    "        models.append(net)\n",
    "        \n",
    "    test_loader = DataLoader(adni_data_test, batch_size=1, num_workers=1, shuffle=False)\n",
    "\n",
    "    ############## INFERENCE\n",
    "    \n",
    "    pred_correct_all = []\n",
    "    balanced_accs = []\n",
    "    raw_preds_all = []\n",
    "    for trial, model in enumerate(models):\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        pred_correct = []\n",
    "        raw_preds = []\n",
    "        \n",
    "        net = model.cuda(gpu)\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            cou = 0\n",
    "            for sample in test_loader:\n",
    "                img = sample[\"image\"]\n",
    "                label = sample[\"label\"]\n",
    "                \n",
    "                img = img.to(torch.device(\"cuda:\" + str(gpu)))\n",
    "                output = net.forward(img)\n",
    "                output_softmax = F.softmax(output, dim=1)\n",
    "                raw_preds.append(output_softmax[0][1].cpu().numpy().item())\n",
    "                pred = torch.argmax(output_softmax)\n",
    "                all_preds.append(pred.cpu().numpy().item())\n",
    "                all_labels.append(label.numpy().item())\n",
    "                pred_correct.append(all_preds[-1] == all_labels[-1])\n",
    "        \n",
    "        raw_preds_all.append(raw_preds)\n",
    "        balanced_accs.append(balanced_accuracy(all_labels, all_preds))\n",
    "        pred_correct_all.append(pred_correct)\n",
    "    \n",
    "    with open('{}/{}/raw_pred.pkl'.format(models_base_path, split), 'wb') as f:\n",
    "        pickle.dump(raw_preds_all, f)\n",
    "    print(\"finished split\", split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
